{\rtf1\ansi\ansicpg1252\cocoartf2511
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red255\green255\blue255;
\red35\green95\blue203;}
{\*\expandedcolortbl;;\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c99985;\cssrgb\c100000\c100000\c99956;
\cssrgb\c17447\c46338\c83489;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 This information comes from a study originally done in 2012 by Jorge L. Reyes-Ortiz, Davide Anguita, Alessandro Ghio, and Luca Oneto at SmarLab.  It takes\cf2  da\cb3 ta from SmartPhones that help to the scientists to study body movements in relation to Hea\cf2 \cb4 lth apps: what does it look like when people sit, stand, walk upstairs, walk downstairs, walk on level ground or lay down.  The original study looks at time and frequency statistics that include 3-D angle measurements, body accelerations and gravity pulls.  Information about the original project and studies can be found at {\field{\*\fldinst{HYPERLINK "http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"}}{\fldrslt \expnd0\expndtw0\kerning0
http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones}}\expnd0\expndtw0\kerning0
 (description) and {\field{\*\fldinst{HYPERLINK "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"}}{\fldrslt https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip}} (data).  A codebook with variable names is also a part of this repo.  The original experiment, off of which my work is based, took 30 people and visually measured the above variables.  70% of the data was designated as training data and 30% as testing data (randomly decided).\
\
My repo contains this README, and the codebook mentioned above and the R script to do replicate the work that I did.  The R script gives detailed instructions and explanations as it walks you through step-by-step what is going on.\
\
I combined the test and training data into one giant data frame.  I then extracted only the variables (columns) that discussed means and standard deviations.  The means and standard deviations included body acceleration and jerk along 3 axes (3-dimensional), gravity acceleration and jerk and angular velocity in 3-D, and all of this was calculated with both time and frequency variables, for a total of 79 extracted variables in the data frame.  There were a total of 10,299 observations on these variables.  The R script detailing all of this is called run_analysis.R.\
\
After creating this data frame, I created one more small data frame that consisted of the mean values for each of the 79 aforementioned variables.  }